{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-9IVeYjDqiZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WCIqgTEC_GU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from scipy.stats import ks_2samp\n",
        "from scipy.stats import skew, kurtosis\n",
        "import python_utils as utils\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input, GRU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "import random\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.layers import LayerNormalization\n",
        "import tensorflow_probability as tfp\n",
        "from scipy.stats import t as student_t\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import initializers\n",
        "import textwrap\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, initializers\n",
        "import tensorflow as tf, os, datetime as dt\n",
        "import tensorflow_probability as tfp\n",
        "tfs = tfp.stats\n",
        "from scipy.stats import wasserstein_distance\n",
        "import os, pickle\n",
        "from scipy.stats import wasserstein_distance\n",
        "import os, shutil, glob, datetime as dt\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "in8qWZW9NhGn"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/SP500_data_fixed.xlsx\"\n",
        "xls = pd.ExcelFile(file_path)\n",
        "\n",
        "print(xls.sheet_names)\n",
        "\n",
        "df = pd.read_excel(xls, sheet_name=0, index_col=0, parse_dates=True)\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "selected_stocks = [\"META\", \"TROW\"]\n",
        "\n",
        "df = df[selected_stocks]\n",
        "\n",
        "df = df.loc[\"2013-01-01\":\"2024-01-01\"]\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "# Save data\n",
        "df.to_csv(\"meta_msft_2013_2019.csv\")\n",
        "\n",
        "csv_filename = \"meta_msft_2013_2019.csv\"\n",
        "df.to_csv(csv_filename)\n",
        "print(f\"Data successfully saved to {csv_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oP5v_MtED9g6"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/SP500_data_fixed.xlsx\"\n",
        "xls = pd.ExcelFile(file_path)\n",
        "\n",
        "print(xls.sheet_names)\n",
        "\n",
        "df = pd.read_excel(xls, sheet_name=0, index_col=0, parse_dates=True)\n",
        "selected_stocks = [\"META\", \"TROW\"]\n",
        "df = df[selected_stocks]\n",
        "df = df.loc[\"2013-01-01\":\"2024-01-01\"]\n",
        "\n",
        "# Compute log returns (stationary transformation)\n",
        "df_log = np.log(df).diff().dropna()\n",
        "\n",
        "# Save the processed data\n",
        "csv_filename = \"meta_msft_logreturns.csv\"\n",
        "df_log.to_csv(csv_filename)\n",
        "print(f\"Data saved to {csv_filename}\")\n",
        "\n",
        "# Check\n",
        "print(df_log.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NogHzpuEAH5"
      },
      "outputs": [],
      "source": [
        "# Reset the index and drop the old index (Date column)\n",
        "df_log.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Now the DataFrame will be indexed by integers (0, 1, 2, ...)\n",
        "print(df_log.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAUo-5WEiPx8"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 6))\n",
        "for stock in selected_stocks:\n",
        "    plt.plot(df.index, df[stock], label=stock)\n",
        "\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Stock Price Sequences (2018-2019)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yScM9VOHEFSV"
      },
      "outputs": [],
      "source": [
        "# def load_and_preprocess(filename, seq_len):\n",
        "#     \"\"\"\n",
        "#     Load your CSV data, sort it in chronological order,\n",
        "#     and segment it into overlapping sequences.\n",
        "\n",
        "#     Args:\n",
        "#       - filename: path to the CSV file.\n",
        "#       - seq_len: the desired sequence length.\n",
        "\n",
        "#     Returns:\n",
        "#       - sequences: a NumPy array of shape (number_of_sequences, seq_len, num_features)\n",
        "#     \"\"\"\n",
        "#     file_path = \"/content/meta_msft_2013_2019.csv\"\n",
        "#     # Load the CSV file (ensure the index is the date)\n",
        "#     df = pd.read_csv(filename, index_col=0, parse_dates=True)\n",
        "#     # Sort by the date to ensure chronological order\n",
        "#     df = df.sort_index()\n",
        "\n",
        "#     # Convert DataFrame to a NumPy array.\n",
        "#     data = df.values  # shape: (num_days, num_features)\n",
        "\n",
        "#     # Create overlapping sequences.\n",
        "#     sequences = []\n",
        "#     for i in range(0, len(data) - seq_len + 1):\n",
        "#         sequences.append(data[i:i + seq_len])\n",
        "#     sequences = np.array(sequences)\n",
        "#     return sequences\n",
        "\n",
        "# # Example usage:\n",
        "# csv_file = \"/content/meta_msft_2013_2019.csv\"\n",
        "# seq_len = 260  # For example, each sequence is 15 days long.\n",
        "# ori_data = load_and_preprocess(csv_file, seq_len)\n",
        "# print(\"Shape of ori_data:\", ori_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JW6b_UYBFzuH"
      },
      "outputs": [],
      "source": [
        "def load_and_preprocess(filename, seq_len):\n",
        "    df = pd.read_csv(filename, index_col=0, parse_dates=True)\n",
        "    df = df.sort_index()\n",
        "    data = df.values\n",
        "\n",
        "    sequences = []\n",
        "    for i in range(len(data) - seq_len + 1):\n",
        "        sequences.append(data[i:i + seq_len])\n",
        "    sequences = np.array(sequences)\n",
        "    return sequences\n",
        "\n",
        "# Usage example\n",
        "csv_file = \"meta_msft_logreturns.csv\"\n",
        "seq_len = 120 # was 60 in the best attempt\n",
        "ori_data = load_and_preprocess(csv_file, seq_len)\n",
        "print(\"Shape of ori_data (log returns):\", ori_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaE_ZdMZEIXo"
      },
      "outputs": [],
      "source": [
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ym_Y2RGkjKeZ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "config = ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.85\n",
        "config.gpu_options.visible_device_list = \"0\"\n",
        "session = InteractiveSession(config=config)\n",
        "\n",
        "# Force TF to use non-CuDNN implementation\n",
        "tf.config.experimental.enable_op_determinism()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEpi1keNDEMs"
      },
      "outputs": [],
      "source": [
        "### Utility Functions ###\n",
        "def train_test_divide (data_x, data_x_hat, data_t, data_t_hat, train_rate = 0.8):\n",
        "  \"\"\"Divide train and test data for both original and synthetic data.\n",
        "\n",
        "  Args:\n",
        "    - data_x: original data\n",
        "    - data_x_hat: generated data\n",
        "    - data_t: original time\n",
        "    - data_t_hat: generated time\n",
        "    - train_rate: ratio of training data from the original data\n",
        "  \"\"\"\n",
        "  # Divide train/test index (original data)\n",
        "  no = len(data_x)\n",
        "  idx = np.random.permutation(no)\n",
        "  train_idx = idx[:int(no*train_rate)]\n",
        "  test_idx = idx[int(no*train_rate):]\n",
        "\n",
        "  train_x = [data_x[i] for i in train_idx]\n",
        "  test_x = [data_x[i] for i in test_idx]\n",
        "  train_t = [data_t[i] for i in train_idx]\n",
        "  test_t = [data_t[i] for i in test_idx]\n",
        "\n",
        "  # Divide train/test index (synthetic data)\n",
        "  no = len(data_x_hat)\n",
        "  idx = np.random.permutation(no)\n",
        "  train_idx = idx[:int(no*train_rate)]\n",
        "  test_idx = idx[int(no*train_rate):]\n",
        "\n",
        "  train_x_hat = [data_x_hat[i] for i in train_idx]\n",
        "  test_x_hat = [data_x_hat[i] for i in test_idx]\n",
        "  train_t_hat = [data_t_hat[i] for i in train_idx]\n",
        "  test_t_hat = [data_t_hat[i] for i in test_idx]\n",
        "\n",
        "  return train_x, train_x_hat, test_x, test_x_hat, train_t, train_t_hat, test_t, test_t_hat\n",
        "\n",
        "def extract_time(data):\n",
        "    \"\"\"Returns Maximum sequence length and each sequence length.\n",
        "\n",
        "  Args:\n",
        "    - data: original data\n",
        "\n",
        "  Returns:\n",
        "    - time: extracted time information\n",
        "    - max_seq_len: maximum sequence length\n",
        "  \"\"\"\n",
        "    time = [len(seq) for seq in data]\n",
        "    max_seq_len = max(time)\n",
        "    return time, max_seq_len\n",
        "\n",
        "def rnn_cell(module_name, hidden_dim):\n",
        "    assert module_name in ['gru', 'lstm']\n",
        "    if module_name == 'gru':\n",
        "        return tf.keras.layers.GRUCell(hidden_dim)\n",
        "    elif module_name == 'lstm':\n",
        "        return tf.keras.layers.LSTMCell(hidden_dim)\n",
        "\n",
        "\n",
        "# def masked_mse(y_true, y_pred, seq_lens):\n",
        "#     \"\"\"Properly handles variable-length sequences\"\"\"\n",
        "#     # Create mask from sequence lengths\n",
        "#     mask = tf.sequence_mask(\n",
        "#         seq_lens,\n",
        "#         maxlen=tf.shape(y_true)[1],\n",
        "#         dtype=tf.float32\n",
        "#     )\n",
        "\n",
        "    # # Expand mask to match feature dimensions\n",
        "    # mask = tf.expand_dims(mask, -1)  # [batch_size, seq_len, 1]\n",
        "\n",
        "    # # Calculate masked MSE\n",
        "    # squared_error = tf.square(y_true - y_pred)\n",
        "    # masked_error = squared_error * mask\n",
        "\n",
        "    # return tf.reduce_sum(masked_error) / (tf.reduce_sum(mask) + 1e-8)\n",
        "\n",
        "\n",
        "def t_noise(shape, df=5, scale=0.6):\n",
        "    \"\"\"t(df) с единичной дисперсией * scale (0.5-0.7 обычно ок).\"\"\"\n",
        "    sigma = np.sqrt(df/(df-2))        # σ исходного t\n",
        "    return scale * student_t.rvs(df, size=shape) / sigma\n",
        "\n",
        "\n",
        "def random_generator(batch_size, z_dim, T_mb, max_seq_len):\n",
        "    \"\"\"Generates a batch of random noise sequences.\"\"\"\n",
        "    Z_mb = np.zeros((batch_size, max_seq_len, z_dim))\n",
        "    for i in range(min(batch_size, len(T_mb))):\n",
        "        seq_length = T_mb[i]\n",
        "        # Z_mb[i, :seq_length, :] = t.rvs(df=3, size=(seq_length, z_dim))\n",
        "        Z_mb[i, :seq_length, :] = t_noise((seq_length, z_dim), df=5, scale=0.6)\n",
        "        # Z_mb[i, :seq_length, :] = np.random.uniform(0., 1., (seq_length, z_dim))\n",
        "    return Z_mb\n",
        "\n",
        "\n",
        "def batch_generator(data, time, rolling_mu, rolling_sigma, crisis_intensities, batch_size):\n",
        "    \"\"\"Modified batch generator that includes crisis intensities\"\"\"\n",
        "    idx = np.random.permutation(len(data))[:batch_size]\n",
        "    seq_lens = [time[i] for i in idx]\n",
        "    max_len = max(seq_lens)\n",
        "\n",
        "    X_mb = np.array([np.pad(seq[:l], ((0,max_len-l),(0,0)), mode='edge')\n",
        "                     for l, seq in zip(seq_lens, data[idx])])\n",
        "    mu_mb = np.array([np.pad(rolling_mu[i][:l], ((0,max_len-l),(0,0)), 'edge')\n",
        "                      for i,l in zip(idx, seq_lens)])\n",
        "    sigma_mb = np.array([np.pad(rolling_sigma[i][:l], ((0,max_len-l),(0,0)), 'edge')\n",
        "                         for i,l in zip(idx, seq_lens)])\n",
        "\n",
        "    # Add crisis intensities\n",
        "    crisis_mb = crisis_intensities[idx].reshape(-1, 1)  # [batch_size, 1]\n",
        "\n",
        "    return X_mb, seq_lens, mu_mb, sigma_mb, crisis_mb\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def exp_decay(initial, final, start_step, end_step, step):\n",
        "    \"\"\"\n",
        "    Exponentially decays a weight from `initial` to `final`\n",
        "    between [start_step, end_step].  Outside that window it is\n",
        "    clamped to the respective edge value.\n",
        "    \"\"\"\n",
        "    step = tf.cast(step, tf.float32)\n",
        "    start_step = tf.cast(start_step, tf.float32)\n",
        "    end_step   = tf.cast(end_step,   tf.float32)\n",
        "\n",
        "    progress = tf.clip_by_value((step - start_step) / (end_step - start_step), 0.0, 1.0)\n",
        "    log_interp = tf.math.log(initial) + progress * (tf.math.log(final) - tf.math.log(initial))\n",
        "    return tf.exp(log_interp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qozpijVyn4yQ"
      },
      "outputs": [],
      "source": [
        "class SNWrapper(tf.keras.layers.Wrapper):\n",
        "    \"\"\"\n",
        "    Spectral Normalisation for every weight matrix in a layer,\n",
        "    including recurrent_kernel for RNN cells.\n",
        "    \"\"\"\n",
        "    def __init__(self, layer, power_iter: int = 1, **kwargs):\n",
        "        super().__init__(layer, **kwargs)\n",
        "        self.power_iter = power_iter\n",
        "        self.w_refs, self.w_shapes, self.u_vecs = [], [], []\n",
        "        self.current_sigma = None\n",
        "\n",
        "    # -------------------------------------------------------------- #\n",
        "    # locate kernels\n",
        "    # -------------------------------------------------------------- #\n",
        "    def build(self, input_shape):\n",
        "        super().build(input_shape)\n",
        "\n",
        "        def _add_kernel(w):\n",
        "            if w is not None:\n",
        "                self.w_refs.append(w)\n",
        "                self.w_shapes.append(w.shape)\n",
        "                self.u_vecs.append(\n",
        "                    self.add_weight(\n",
        "                        shape=(1, w.shape[-1]),\n",
        "                        initializer=\"random_normal\",\n",
        "                        trainable=False,\n",
        "                        name=f\"sn_u_{len(self.u_vecs)}\",\n",
        "                    )\n",
        "                )\n",
        "\n",
        "        _add_kernel(getattr(self.layer, \"kernel\", None))\n",
        "\n",
        "        cell = getattr(self.layer, \"cell\", None)\n",
        "        if cell is not None:                       # LSTM/GRU wrapper\n",
        "            _add_kernel(getattr(cell, \"kernel\", None))\n",
        "            _add_kernel(getattr(cell, \"recurrent_kernel\", None))\n",
        "        else:                                      # bare RNN cell\n",
        "            _add_kernel(getattr(self.layer, \"recurrent_kernel\", None))\n",
        "\n",
        "        if not self.w_refs:\n",
        "            raise ValueError(f\"No kernel found in layer {self.layer.name}\")\n",
        "\n",
        "    # -------------------------------------------------------------- #\n",
        "    # spectral norm for a single matrix\n",
        "    # -------------------------------------------------------------- #\n",
        "    @staticmethod\n",
        "    def _spectral_norm(w, u_var, power_iter):\n",
        "        w_mat = tf.reshape(w, [-1, w.shape[-1]])\n",
        "        u_hat = u_var\n",
        "        for _ in range(power_iter):\n",
        "            v_hat = tf.linalg.l2_normalize(tf.matmul(u_hat, w_mat, transpose_b=True))\n",
        "            u_hat = tf.linalg.l2_normalize(tf.matmul(v_hat, w_mat))\n",
        "        sigma = tf.matmul(tf.matmul(v_hat, w_mat), u_hat, transpose_b=True)\n",
        "        u_var.assign(u_hat)\n",
        "        return tf.reshape(w_mat / sigma, w.shape)\n",
        "\n",
        "    # -------------------------------------------------------------- #\n",
        "    # forward pass\n",
        "    # -------------------------------------------------------------- #\n",
        "    def call(self, inputs, training=None, **kwargs):\n",
        "        originals = [tf.identity(w) for w in self.w_refs]   # <— fixed here\n",
        "\n",
        "        for w, u in zip(self.w_refs, self.u_vecs):\n",
        "            w.assign(self._spectral_norm(w, u, self.power_iter))\n",
        "\n",
        "        out = self.layer(inputs, training=training, **kwargs)\n",
        "\n",
        "        for w, w_orig in zip(self.w_refs, originals):\n",
        "            w.assign(w_orig)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return self.layer.compute_output_shape(input_shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JPD2NPSEMWM"
      },
      "outputs": [],
      "source": [
        "dim = ori_data.shape[2]  # Extract the number of features (columns)\n",
        "max_seq_len = seq_len\n",
        "z_dim = 8 # Latent space dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8US3-vykHFxi"
      },
      "outputs": [],
      "source": [
        "parameters = {\n",
        "    'hidden_dim': 64, # Number of hidden units in the RNN layers, was 64 in the best attempt\n",
        "    'num_layer': 3, # Number of RNN layers\n",
        "    'iterations': 20000, # Number of training iterations\n",
        "    'batch_size': 64, # Size of each batch during training !!!!\n",
        "    'z_dim': 10, # Latent space dimension\n",
        "    'module': 'gru' # Define the RNN module ('lstm')\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGteffYrERNG"
      },
      "outputs": [],
      "source": [
        "# Network Parameters\n",
        "hidden_dim = parameters['hidden_dim']\n",
        "num_layers = parameters['num_layer']\n",
        "iterations = parameters['iterations']\n",
        "batch_size = parameters['batch_size']\n",
        "module_name = parameters['module']\n",
        "z_dim = parameters['z_dim']\n",
        "gamma = 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VolatilityScaler(tf.keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "                 z_dim: int,\n",
        "                 init_scale: float = 1.2,\n",
        "                 init_sig: tf.Tensor | None = None):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        z_dim      : latent-feature dimensionality\n",
        "        init_scale : fallback scale if `init_sig` is None\n",
        "        init_sig   : 1-D tensor of length z_dim with the target\n",
        "                     per-feature σ of real returns (may be NumPy\n",
        "                     array – it is converted to tf.constant)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.z_dim      = z_dim\n",
        "        self.init_scale = init_scale\n",
        "        self.init_sig   = (tf.constant(init_sig, dtype=tf.float32)\n",
        "                           if init_sig is not None else None)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3 and input_shape[-1] == self.z_dim, \\\n",
        "            f\"expected shape (batch, time, {self.z_dim})\"\n",
        "\n",
        "        if self.init_sig is not None:\n",
        "            # Convert to numpy before using in initializer\n",
        "            init_value = tf.math.log(self.init_sig)[None, None, :].numpy()\n",
        "        else:\n",
        "            # For scalar fallback, create numpy array directly\n",
        "            init_value = tf.fill([1, 1, self.z_dim],\n",
        "                              tf.math.log(self.init_scale)).numpy()\n",
        "\n",
        "        self.log_scale = self.add_weight(\n",
        "            name=\"log_scale\",\n",
        "            shape=(1, 1, self.z_dim),\n",
        "            initializer=tf.constant_initializer(init_value),\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "    def call(self, z):\n",
        "        return z * tf.exp(self.log_scale)"
      ],
      "metadata": {
        "id": "uagTTQxbeJht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex2sj0wywA--"
      },
      "outputs": [],
      "source": [
        "class SelfAttentionPooling(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_dim, **kwargs):\n",
        "        super(SelfAttentionPooling, self).__init__(**kwargs)\n",
        "        self.W = Dense(1, use_bias=False)\n",
        "\n",
        "    def call(self, batch_rep):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            batch_rep: (batch_size, seq_len, hidden_dim)\n",
        "        Returns:\n",
        "            pooled_rep: (batch_size, hidden_dim)\n",
        "        \"\"\"\n",
        "        # Compute attention weights\n",
        "        att_scores = self.W(batch_rep)                    # (B,T,1)\n",
        "        att_weights = tf.nn.softmax(att_scores, axis=1)   # (B,T,1)\n",
        "\n",
        "        # Apply attention weights\n",
        "        pooled_rep = tf.reduce_sum(batch_rep * att_weights, axis=1)  # (B,H)\n",
        "        return pooled_rep"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install arch"
      ],
      "metadata": {
        "id": "WFYYXW9SmuF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this after your existing imports\n",
        "import numpy as np\n",
        "from arch import arch_model\n",
        "\n",
        "import warnings\n",
        "from arch.univariate.base import ConvergenceWarning\n",
        "import warnings\n",
        "from arch.univariate.base import ConvergenceWarning\n",
        "import pandas as pd\n",
        "\n",
        "def estimate_crisis_intensity(returns_series, window_size=252):\n",
        "    \"\"\"\n",
        "    Estimate crisis intensity parameter using EGARCH conditional volatility with fallback\n",
        "    \"\"\"\n",
        "    # Suppress convergence warnings\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
        "\n",
        "        try:\n",
        "            # Try EGARCH first\n",
        "            model = arch_model(returns_series, vol='EGARCH', p=1, o=1, q=1, dist='t', rescale=False)\n",
        "            results = model.fit(disp='off', options={'maxiter': 500})\n",
        "\n",
        "            # Check if convergence was successful\n",
        "            if results.convergence_flag != 0:\n",
        "                raise ValueError(\"EGARCH did not converge properly\")\n",
        "\n",
        "            conditional_volatility = results.conditional_volatility\n",
        "\n",
        "            # Convert to numpy array if it's a pandas Series\n",
        "            if hasattr(conditional_volatility, 'values'):\n",
        "                conditional_volatility = conditional_volatility.values\n",
        "\n",
        "            # Handle NaN values\n",
        "            conditional_volatility = np.nan_to_num(conditional_volatility, nan=0.0)\n",
        "\n",
        "            # Normalize to [0,1] range\n",
        "            sigma_min = np.min(conditional_volatility)\n",
        "            sigma_max = np.max(conditional_volatility)\n",
        "\n",
        "            if sigma_max - sigma_min == 0:\n",
        "                crisis_intensity = np.zeros_like(conditional_volatility)\n",
        "            else:\n",
        "                crisis_intensity = (conditional_volatility - sigma_min) / (sigma_max - sigma_min)\n",
        "\n",
        "            return crisis_intensity, sigma_min, sigma_max\n",
        "\n",
        "        except:\n",
        "            # Fallback to simple GARCH if EGARCH fails\n",
        "            try:\n",
        "                model = arch_model(returns_series, vol='GARCH', p=1, q=1, rescale=False)\n",
        "                results = model.fit(disp='off')\n",
        "                conditional_volatility = results.conditional_volatility\n",
        "\n",
        "                if hasattr(conditional_volatility, 'values'):\n",
        "                    conditional_volatility = conditional_volatility.values\n",
        "\n",
        "                conditional_volatility = np.nan_to_num(conditional_volatility, nan=0.0)\n",
        "\n",
        "                sigma_min = np.min(conditional_volatility)\n",
        "                sigma_max = np.max(conditional_volatility)\n",
        "\n",
        "                if sigma_max - sigma_min == 0:\n",
        "                    crisis_intensity = np.zeros_like(conditional_volatility)\n",
        "                else:\n",
        "                    crisis_intensity = (conditional_volatility - sigma_min) / (sigma_max - sigma_min)\n",
        "\n",
        "                return crisis_intensity, sigma_min, sigma_max\n",
        "\n",
        "            except:\n",
        "                # Final fallback: rolling volatility\n",
        "                return estimate_crisis_intensity_simple(returns_series, window_size)\n",
        "\n",
        "def estimate_crisis_intensity_simple(returns_series, window_size=252):\n",
        "    \"\"\"\n",
        "    Simple crisis intensity using rolling volatility (fallback method)\n",
        "    \"\"\"\n",
        "    # Calculate rolling volatility\n",
        "    returns_df = pd.Series(returns_series)\n",
        "    rolling_vol = returns_df.rolling(window=20, min_periods=5).std()\n",
        "    rolling_vol = rolling_vol.fillna(rolling_vol.mean())\n",
        "\n",
        "    # Normalize to [0,1]\n",
        "    sigma_min = rolling_vol.min()\n",
        "    sigma_max = rolling_vol.max()\n",
        "\n",
        "    if sigma_max - sigma_min == 0:\n",
        "        crisis_intensity = np.zeros_like(rolling_vol)\n",
        "    else:\n",
        "        crisis_intensity = (rolling_vol - sigma_min) / (sigma_max - sigma_min)\n",
        "\n",
        "    return crisis_intensity.values, sigma_min, sigma_max\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_with_crisis_intensity(data, window=252):\n",
        "    \"\"\"\n",
        "    Preprocess data and estimate crisis intensity for each sequence\n",
        "    \"\"\"\n",
        "    crisis_intensities = []\n",
        "    sigma_mins = []\n",
        "    sigma_maxs = []\n",
        "\n",
        "    for seq_idx in range(len(data)):\n",
        "        # Get returns for this sequence (flatten across features)\n",
        "        seq_returns = data[seq_idx].flatten()\n",
        "\n",
        "        # Estimate crisis intensity\n",
        "        c_values, sigma_min, sigma_max = estimate_crisis_intensity(seq_returns)\n",
        "\n",
        "        # Take mean crisis intensity for the sequence\n",
        "        mean_crisis = np.mean(c_values) if len(c_values) > 0 else 0.0\n",
        "        crisis_intensities.append(mean_crisis)\n",
        "        sigma_mins.append(sigma_min)\n",
        "        sigma_maxs.append(sigma_max)\n",
        "\n",
        "    return np.array(crisis_intensities), np.array(sigma_mins), np.array(sigma_maxs)"
      ],
      "metadata": {
        "id": "JZPP-zlEfhq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConditionalGenerator(tf.keras.Model):\n",
        "    def __init__(self, hidden_dim, z_dim, num_layers=3):\n",
        "        super(ConditionalGenerator, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.z_dim = z_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Crisis intensity embedding layer\n",
        "        self.crisis_embedding = tf.keras.layers.Dense(hidden_dim//4, activation='relu')\n",
        "\n",
        "        # Volatility scaler\n",
        "        self.volatility_scaler = VolatilityScaler(z_dim)\n",
        "\n",
        "        # LSTM stack\n",
        "        self.lstm_layers = []\n",
        "        for i in range(num_layers):\n",
        "            self.lstm_layers.append(\n",
        "                tf.keras.layers.LSTM(\n",
        "                    hidden_dim,\n",
        "                    return_sequences=True,\n",
        "                    activation='tanh',\n",
        "                    recurrent_activation='sigmoid'\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Output projection\n",
        "        self.output_layer = tf.keras.layers.Dense(hidden_dim, activation='linear')\n",
        "\n",
        "    def call(self, noise_input, crisis_intensity, training=None):\n",
        "        batch_size = tf.shape(noise_input)[0]\n",
        "        seq_length = tf.shape(noise_input)[1]\n",
        "\n",
        "        # Apply volatility scaling to noise\n",
        "        scaled_noise = self.volatility_scaler(noise_input)\n",
        "\n",
        "        # Embed crisis intensity\n",
        "        c_embedding = self.crisis_embedding(crisis_intensity)  # [batch, emb_dim]\n",
        "\n",
        "        # Repeat crisis embedding for each time step\n",
        "        c_repeated = tf.tile(\n",
        "            tf.expand_dims(c_embedding, 1),\n",
        "            [1, seq_length, 1]\n",
        "        )  # [batch, seq_length, emb_dim]\n",
        "\n",
        "        # Concatenate noise with crisis conditioning\n",
        "        combined_input = tf.concat([scaled_noise, c_repeated], axis=-1)\n",
        "\n",
        "        # Pass through LSTM stack\n",
        "        hidden_states = combined_input\n",
        "        for lstm_layer in self.lstm_layers:\n",
        "            hidden_states = lstm_layer(hidden_states, training=training)\n",
        "\n",
        "        # Generate output\n",
        "        output = self.output_layer(hidden_states)\n",
        "        return output"
      ],
      "metadata": {
        "id": "j0R8RF-gfoXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConditionalDiscriminator(tf.keras.Model):\n",
        "    def __init__(self, hidden_dim, num_layers=3):\n",
        "        super(ConditionalDiscriminator, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # LSTM backbone with spectral normalization\n",
        "        self.lstm_layers = []\n",
        "        for i in range(num_layers):\n",
        "            lstm = tf.keras.layers.LSTM(\n",
        "                hidden_dim,\n",
        "                return_sequences=True,\n",
        "                activation='tanh',\n",
        "                recurrent_activation='sigmoid',\n",
        "                unroll=True\n",
        "            )\n",
        "            # Apply spectral normalization to LSTM weights\n",
        "            lstm = SNWrapper(lstm, power_iter=1)\n",
        "            self.lstm_layers.append(lstm)\n",
        "\n",
        "        # Self-attention pooling\n",
        "        self.attention_pooling = SelfAttentionPooling(hidden_dim)\n",
        "\n",
        "        # Dual heads for ACGAN approach\n",
        "        self.adversarial_head = SNWrapper(\n",
        "            tf.keras.layers.Dense(1, use_bias=False),\n",
        "            power_iter=1\n",
        "        )  # Real/fake classification\n",
        "        self.crisis_regression_head = tf.keras.layers.Dense(\n",
        "            1, activation='sigmoid'\n",
        "        )  # Crisis intensity regression\n",
        "\n",
        "    def call(self, latent_input, training=None):\n",
        "        # Pass through LSTM stack\n",
        "        hidden_states = latent_input\n",
        "        for lstm_layer in self.lstm_layers:\n",
        "            hidden_states = lstm_layer(hidden_states, training=training)\n",
        "\n",
        "        # Apply self-attention pooling\n",
        "        pooled_rep = self.attention_pooling(hidden_states)\n",
        "\n",
        "        # Dual outputs\n",
        "        adversarial_output = self.adversarial_head(pooled_rep)\n",
        "        crisis_output = self.crisis_regression_head(pooled_rep)\n",
        "\n",
        "        return adversarial_output, crisis_output"
      ],
      "metadata": {
        "id": "2OCL7EMGfrVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivvrSs4vETCO"
      },
      "outputs": [],
      "source": [
        "class TimeGAN:\n",
        "    def __init__(self, rolling_stats, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.rolling_stats = rolling_stats\n",
        "        self.hidden_dim = parameters['hidden_dim']\n",
        "        self.num_layers = parameters['num_layer']\n",
        "        self.iterations = parameters['iterations']\n",
        "        self.batch_size = parameters['batch_size']\n",
        "        self.module_name = parameters['module']\n",
        "        self.z_dim = parameters['z_dim']\n",
        "        self.gamma = 1  # Adversarial weight\n",
        "        self.dim = 2  # Dimensionality of input data (e.g., two stocks)\n",
        "\n",
        "        # Build all sub-models first\n",
        "        self.embedder_model = self.build_embedder()\n",
        "        self.recovery_model = self.build_recovery()\n",
        "        self.supervisor_model = self.build_supervisor()\n",
        "        # Store crisis intensities\n",
        "        self.crisis_intensities = crisis_intensities\n",
        "        # Replace generator and discriminator with conditional versions\n",
        "        self.generator_model = self.build_conditional_generator()\n",
        "        self.discriminator_model = self.build_conditional_discriminator()\n",
        "        # Define optimizers\n",
        "        self.optimizer_E = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "        self.optimizer_R = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "        self.optimizer_S = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "        self.optimizer_G = tf.keras.optimizers.Adam(learning_rate=1e-4,\n",
        "                                            beta_1=0.5, beta_2=0.9)\n",
        "        self.optimizer_D = tf.keras.optimizers.RMSprop(learning_rate=3e-5)\n",
        "\n",
        "        # self.optimizer_D = tf.keras.optimizers.Adam(learning_rate=1e-4,\n",
        "        #                                     beta_1=0.5, beta_2=0.9)\n",
        "\n",
        "        # self.optimizer_G = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "        # self.optimizer_D = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
        "\n",
        "        self.mse_loss = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "        # Dummy initialization (after models are defined)\n",
        "        print(\"Initializing model weights...\")\n",
        "        dummy_X = tf.random.normal([1, 60, self.dim])\n",
        "        dummy_Z = tf.random.normal([1, 60, self.z_dim])\n",
        "        dummy_H = tf.random.normal([1, 60, self.hidden_dim])\n",
        "        _ = self.embedder_model(dummy_X)\n",
        "        _ = self.recovery_model(dummy_H)\n",
        "        _ = self.generator_model(dummy_Z)\n",
        "        _ = self.supervisor_model(dummy_H)\n",
        "        _ = self.discriminator_model(dummy_H)\n",
        "        print(\"Model weights initialized.\")\n",
        "\n",
        "    def build_embedder(self):\n",
        "        inputs = Input(shape=(None, self.dim))\n",
        "        x = inputs\n",
        "        for _ in range(self.num_layers):\n",
        "            x = LSTM(self.hidden_dim, return_sequences=True)(x)\n",
        "            #  x = layers.GRU(self.hidden_dim, return_sequences=True)(x)\n",
        "        # outputs = Dense(self.hidden_dim, activation='sigmoid')(x) #was in the best attempt\n",
        "        outputs = Dense(self.hidden_dim, activation='linear')(x)\n",
        "        # outputs = Dense(self.hidden_dim, activation='tanh')(x)\n",
        "        return tf.keras.Model(inputs, outputs, name='Embedder')\n",
        "\n",
        "    def build_recovery(self):\n",
        "        inputs = tf.keras.Input((None, self.hidden_dim))\n",
        "        x = inputs\n",
        "        for _ in range(self.num_layers):\n",
        "            x = tf.keras.layers.LSTM(self.hidden_dim,\n",
        "                                  return_sequences=True,\n",
        "                                  activation='swish')(x)  # Better gradient flow\n",
        "        # Direct output without artificial scaling\n",
        "        outputs = tf.keras.layers.Dense(self.dim, activation='linear')(x)\n",
        "        return tf.keras.Model(inputs, outputs, name=\"Recovery\")\n",
        "\n",
        "    def build_supervisor(self):\n",
        "        inputs = Input(shape=(None, self.hidden_dim))\n",
        "        x = inputs\n",
        "        for _ in range(self.num_layers - 1):\n",
        "            x = LSTM(self.hidden_dim, return_sequences=True)(x)\n",
        "            #  x = layers.GRU(self.hidden_dim, return_sequences=True)(x)\n",
        "        # outputs = Dense(self.hidden_dim, activation='sigmoid')(x)\n",
        "        # outputs = Dense(self.hidden_dim, activation='tanh')(x)\n",
        "        outputs = Dense(self.hidden_dim, activation='linear')(x)\n",
        "        return tf.keras.Model(inputs, outputs, name='Supervisor')\n",
        "\n",
        "    def build_conditional_generator(self):\n",
        "        return ConditionalGenerator(\n",
        "            hidden_dim=self.hidden_dim,\n",
        "            z_dim=self.z_dim,\n",
        "            num_layers=self.num_layers\n",
        "        )\n",
        "\n",
        "    def build_conditional_discriminator(self):\n",
        "        return ConditionalDiscriminator(\n",
        "            hidden_dim=self.hidden_dim,\n",
        "            num_layers=self.num_layers\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jf28ML2QXW71"
      },
      "outputs": [],
      "source": [
        "# Extract time information at the beginning\n",
        "ori_time, max_seq_len = extract_time(ori_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XawOxm4E2DhT"
      },
      "outputs": [],
      "source": [
        "def rolling_window_standardize(data, window=252):\n",
        "    \"\"\"\n",
        "    Faster rolling standardization using pandas.\n",
        "\n",
        "    Args:\n",
        "        data: (num_sequences, seq_len, num_features) numpy array\n",
        "        window: rolling window size\n",
        "\n",
        "    Returns:\n",
        "        standardized_data: same shape as input\n",
        "        rolling_stats: statistics for descaling\n",
        "    \"\"\"\n",
        "    num_sequences, seq_len, num_features = data.shape\n",
        "    standardized_data = np.zeros_like(data)\n",
        "    rolling_means = np.zeros_like(data)\n",
        "    rolling_stds = np.zeros_like(data)\n",
        "\n",
        "    for feat_idx in range(num_features):\n",
        "        for seq_idx in range(num_sequences):\n",
        "            # Convert to pandas Series for efficient rolling operations\n",
        "            series = pd.Series(data[seq_idx, :, feat_idx])\n",
        "\n",
        "            # Compute rolling statistics with minimum periods\n",
        "            rolling_mean = series.rolling(window=window, min_periods=10).mean()\n",
        "            rolling_std = series.rolling(window=window, min_periods=10).std()\n",
        "\n",
        "            # Fill initial NaN values with expanding window\n",
        "            rolling_mean = rolling_mean.fillna(series.expanding().mean())\n",
        "            rolling_std = rolling_std.fillna(series.expanding().std())\n",
        "            rolling_std = rolling_std.fillna(1e-8).clip(lower=1e-8)\n",
        "\n",
        "            # Store statistics\n",
        "            rolling_means[seq_idx, :, feat_idx] = rolling_mean.values\n",
        "            rolling_stds[seq_idx, :, feat_idx] = rolling_std.values\n",
        "\n",
        "            # Standardize\n",
        "            standardized_data[seq_idx, :, feat_idx] = (\n",
        "                (series - rolling_mean) / rolling_std\n",
        "            ).values\n",
        "\n",
        "    return standardized_data, {\n",
        "        'means': rolling_means,\n",
        "        'stds': rolling_stds\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkGCvF37VE5H"
      },
      "outputs": [],
      "source": [
        "def preprocess_data_rolling(data, window=252, train_ratio=0.8):\n",
        "    \"\"\"\n",
        "    Split data and apply rolling window standardization separately.\n",
        "\n",
        "    Args:\n",
        "        data: (num_sequences, seq_len, num_features) numpy array\n",
        "        window: rolling window size for standardization\n",
        "        train_ratio: fraction of data for training\n",
        "\n",
        "    Returns:\n",
        "        train_data: standardized training data\n",
        "        test_data: standardized test data\n",
        "        train_stats: rolling statistics for training data\n",
        "        test_stats: rolling statistics for test data\n",
        "    \"\"\"\n",
        "    # Split first, then standardize separately\n",
        "    n_sequences = len(data)\n",
        "    split_idx = int(n_sequences * train_ratio)\n",
        "\n",
        "    train_raw = data[:split_idx]\n",
        "    test_raw = data[split_idx:]\n",
        "\n",
        "    # Apply rolling standardization separately\n",
        "    train_data, train_stats = rolling_window_standardize(train_raw, window)\n",
        "    test_data, test_stats = rolling_window_standardize(test_raw, window)\n",
        "\n",
        "    return train_data, test_data, train_stats, test_stats\n",
        "\n",
        "# Usage in your main pipeline\n",
        "train_data, test_data, train_stats, test_stats = preprocess_data_rolling(\n",
        "    ori_data, window=252, train_ratio=0.8\n",
        ")\n",
        "\n",
        "# Update your global variables\n",
        "ori_data = np.concatenate([train_data, test_data], axis=0)\n",
        "ori_time, max_seq_len = extract_time(ori_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDs4-6kJDJnz"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------\n",
        "# rolling-stat tensors that every batch-generator call will re-use\n",
        "# ------------------------------------------------------------------\n",
        "roll_means = np.concatenate([train_stats['means'],  test_stats['means']], 0)\n",
        "roll_stds  = np.concatenate([train_stats['stds'],   test_stats['stds']],  0)\n",
        "\n",
        "rolling_stats = {\n",
        "    \"means\": roll_means,   # shape (N, L, F)\n",
        "    \"stds\" : roll_stds     # same shape\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimate crisis intensities for your data\n",
        "print(\"Estimating crisis intensities...\")\n",
        "crisis_intensities = []\n",
        "for seq_idx in range(len(ori_data)):\n",
        "    # Get returns for this sequence (flatten across features)\n",
        "    seq_returns = ori_data[seq_idx].flatten()\n",
        "\n",
        "    # Estimate crisis intensity using EGARCH\n",
        "    c_values, sigma_min, sigma_max = estimate_crisis_intensity(seq_returns)\n",
        "\n",
        "    # Take mean crisis intensity for the sequence\n",
        "    mean_crisis = np.mean(c_values) if len(c_values) > 0 else 0.0\n",
        "    crisis_intensities.append(mean_crisis)\n",
        "\n",
        "crisis_intensities = np.array(crisis_intensities)\n",
        "print(f\"Crisis intensities range: {crisis_intensities.min():.3f} to {crisis_intensities.max():.3f}\")\n"
      ],
      "metadata": {
        "id": "cGjMUiZdoixp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TimeGAN(rolling_stats)"
      ],
      "metadata": {
        "id": "7vYS3sTaoGZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hc9IdQrQ3lcr"
      },
      "outputs": [],
      "source": [
        "def descale_rolling(x_scaled, rolling_stats, is_training=True):\n",
        "    \"\"\"\n",
        "    Descale using rolling window statistics.\n",
        "\n",
        "    Args:\n",
        "        x_scaled: (batch_size, seq_len, num_features) scaled data\n",
        "        rolling_stats: dict with 'means' and 'stds' arrays\n",
        "        is_training: whether to use training or test statistics\n",
        "\n",
        "    Returns:\n",
        "        descaled data in original scale\n",
        "    \"\"\"\n",
        "    stats_key = 'train' if is_training else 'test'\n",
        "    means = rolling_stats[stats_key]['means']\n",
        "    stds = rolling_stats[stats_key]['stds']\n",
        "\n",
        "    # Convert to TensorFlow tensors if needed\n",
        "    if isinstance(x_scaled, tf.Tensor):\n",
        "        means_tf = tf.constant(means, dtype=x_scaled.dtype)\n",
        "        stds_tf = tf.constant(stds, dtype=x_scaled.dtype)\n",
        "\n",
        "        # For batch processing, tile statistics to match batch size\n",
        "        batch_size = tf.shape(x_scaled)[0]\n",
        "        means_tiled = tf.tile(tf.expand_dims(means_tf, 0), [batch_size, 1, 1])\n",
        "        stds_tiled = tf.tile(tf.expand_dims(stds_tf, 0), [batch_size, 1, 1])\n",
        "\n",
        "        return x_scaled * stds_tiled + means_tiled\n",
        "    else:\n",
        "        # NumPy version\n",
        "        return x_scaled * stds + means\n",
        "\n",
        "# Store rolling statistics globally for use in model\n",
        "global_rolling_stats = {\n",
        "    'train': train_stats,\n",
        "    'test': test_stats\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hajFyGsyEe3N"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_embedder(X_mb, T_mb):\n",
        "    with tf.GradientTape() as tape:\n",
        "        H = model.embedder_model(X_mb)  # Encode original data\n",
        "        X_tilde = model.recovery_model(H)  # Decode back to original space\n",
        "        E_loss_T0 = tf.reduce_mean(model.mse_loss(X_mb, X_tilde))  # Reconstruction Loss\n",
        "        # E_loss_T0 = masked_mse(X_mb, X_tilde, T_mb)\n",
        "        E_loss = 10 * tf.sqrt(E_loss_T0)  # Maintain loss scaling\n",
        "\n",
        "    gradients = tape.gradient(E_loss,\n",
        "                              model.embedder_model.trainable_variables +\n",
        "                              model.recovery_model.trainable_variables)\n",
        "\n",
        "    gradients, _ = tf.clip_by_global_norm(gradients, 0.5)  # Prevent exploding gradients\n",
        "\n",
        "    # Apply gradient updates for both the embedder and recovery networks\n",
        "    model.optimizer_E.apply_gradients(zip(gradients,\n",
        "                                          model.embedder_model.trainable_variables +\n",
        "                                          model.recovery_model.trainable_variables))\n",
        "\n",
        "    return E_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XffY0ZqLJK3z"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_embedder_step_joint(X_mb: tf.Tensor, T_mb: tf.Tensor) -> tf.Tensor:\n",
        "    \"\"\"\n",
        "    Joint training embedder step (optional).\n",
        "    We continue fine-tuning the embedder + recovery networks\n",
        "    to improve reconstruction.\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Encode real data into H\n",
        "        H = model.embedder_model(X_mb)\n",
        "        # Decode back to original space\n",
        "        X_tilde = model.recovery_model(H)\n",
        "        # Reconstruction Loss\n",
        "        E_loss_T0 = model.mse_loss(X_mb, X_tilde)\n",
        "        # E_loss_T0 = masked_mse(X_mb, X_tilde, T_mb)\n",
        "        # Scale up the embedder loss slightly\n",
        "        E_loss = 10 * tf.sqrt(E_loss_T0 + 1e-9)\n",
        "\n",
        "    # Compute gradients\n",
        "    e_vars = model.embedder_model.trainable_variables + model.recovery_model.trainable_variables\n",
        "    e_grads = tape.gradient(E_loss, e_vars)\n",
        "    e_grads, _ = tf.clip_by_global_norm(e_grads, 0.5)\n",
        "\n",
        "    # Apply gradients\n",
        "    model.optimizer_E.apply_gradients(zip(e_grads, e_vars))\n",
        "\n",
        "    return E_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADicrUCWFBQy"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_supervised_step(X_mb, T_mb):\n",
        "    \"\"\"Train the supervisor network to predict next latent state.\"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        H = model.embedder_model(X_mb)\n",
        "        H_hat_supervise = model.supervisor_model(H)\n",
        "        G_loss_S = tf.reduce_mean(tf.keras.losses.MeanSquaredError()(H[:, :-1, :], H_hat_supervise[:, 1:, :]))\n",
        "\n",
        "    gradients = tape.gradient(G_loss_S, model.supervisor_model.trainable_variables)\n",
        "    model.optimizer_S.apply_gradients(zip(gradients, model.supervisor_model.trainable_variables))\n",
        "    return G_loss_S"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-cls3nqeZQA"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "# SAFE batched_acf  – works inside @tf.function\n",
        "# ------------------------------------------------------------\n",
        "@tf.function\n",
        "def batched_acf(x, max_lag: int):\n",
        "    \"\"\"\n",
        "    x : (B,T,F) float32/64\n",
        "    returns tensor shape (max_lag, F) – ACF averaged over batch\n",
        "    \"\"\"\n",
        "    B = tf.shape(x)[0]\n",
        "    T = tf.shape(x)[1]\n",
        "    F = tf.shape(x)[2]\n",
        "\n",
        "    x_mean = tf.reduce_mean(x, axis=1, keepdims=True)\n",
        "    xc     = x - x_mean\n",
        "    var    = tf.reduce_mean(tf.square(xc), axis=1, keepdims=True)   # (B,1,F)\n",
        "    var    = tf.maximum(var, 1e-8)\n",
        "\n",
        "    # TensorArray to collect results inside the loop\n",
        "    ta = tf.TensorArray(dtype=x.dtype, size=max_lag, dynamic_size=False,\n",
        "                        clear_after_read=False)\n",
        "\n",
        "    T_f = tf.cast(T, x.dtype)\n",
        "\n",
        "    def body(k, ta_):\n",
        "        num = tf.reduce_sum(xc[:, :-k, :] * xc[:, k:, :], axis=1)           # (B,F)\n",
        "        denom = (T_f - tf.cast(k, x.dtype)) * var[:, 0, :]                  # (B,F)\n",
        "        ta_ = ta_.write(k-1, tf.reduce_mean(num / denom, axis=0))           # (F,)\n",
        "        return k+1, ta_\n",
        "\n",
        "    _, ta_final = tf.while_loop(\n",
        "        lambda k, *_: k <= max_lag,\n",
        "        body,\n",
        "        loop_vars=(1, ta)\n",
        "    )\n",
        "\n",
        "    # stack -> (max_lag, F)\n",
        "    return ta_final.stack()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tail_loss_two_sided(r_real, r_fake, q=0.05,\n",
        "                        w_left=1., w_right=.5):\n",
        "    rr = tf.reshape(r_real, [-1, tf.shape(r_real)[-1]])\n",
        "    rf = tf.reshape(r_fake, [-1, tf.shape(r_fake)[-1]])\n",
        "\n",
        "    # ---- VaR/ES of the REAL distribution only -----------------\n",
        "    VaR_l = tfp.stats.percentile(rr, 100*q,      axis=0)\n",
        "    VaR_r = tfp.stats.percentile(rr, 100*(1-q),  axis=0)\n",
        "\n",
        "    ES_r_l = tf.reduce_mean(tf.boolean_mask(rr, rr <= VaR_l), axis=0)\n",
        "    ES_f_l = tf.reduce_mean(tf.boolean_mask(rf, rf <= VaR_l), axis=0)  # <-- SAME threshold\n",
        "\n",
        "    ES_r_r = tf.reduce_mean(tf.boolean_mask(rr, rr >= VaR_r), axis=0)\n",
        "    ES_f_r = tf.reduce_mean(tf.boolean_mask(rf, rf >= VaR_r), axis=0)  # <-- SAME threshold\n",
        "\n",
        "    sig = tf.math.reduce_std(rr, axis=0) + 1e-8          # real σ\n",
        "\n",
        "    loss_left  = tf.square((ES_r_l - ES_f_l) / sig)\n",
        "    loss_right = tf.square((ES_r_r - ES_f_r) / sig)\n",
        "\n",
        "    return tf.reduce_mean(w_left*loss_left + w_right*loss_right)"
      ],
      "metadata": {
        "id": "3LlE8zdscERQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rolling_std(x: tf.Tensor, win: int, mask: tf.Tensor = None):\n",
        "    \"\"\"Causal rolling σ over last *win* steps.\n",
        "    If *mask* is supplied (shape == x), treats 0s as missing.\n",
        "    Args\n",
        "    -----\n",
        "    x    : (B,T,F) float32/64  – returns in real space\n",
        "    win  : int                  – window length\n",
        "    mask : (B,T,1|F) or None   – 0/1 mask\n",
        "    \"\"\"\n",
        "    if mask is None:\n",
        "        mask = tf.ones_like(x)\n",
        "    w   = tf.cast(mask, x.dtype)\n",
        "    num = tf.nn.avg_pool1d(x * w, win, 1, \"SAME\", data_format=\"NWC\")\n",
        "    den = tf.nn.avg_pool1d(w,      win, 1, \"SAME\", data_format=\"NWC\") + 1e-8\n",
        "    mean = num / den\n",
        "    var  = tf.nn.avg_pool1d(w * tf.square(x - mean), win, 1, \"SAME\", data_format=\"NWC\") / den\n",
        "    return tf.sqrt(var + 1e-7)"
      ],
      "metadata": {
        "id": "AP4ipgbinYCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfLnWYZEt29T"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "# heteroskedasticity loss\n",
        "# ------------------------------------------------------------\n",
        "def heteroskedasticity_loss(real_r: tf.Tensor, fake_r: tf.Tensor, window: int = 20):\n",
        "    \"\"\"L1 difference of rolling σ  +  ACF of squared returns.\"\"\"\n",
        "    real_vol = rolling_std(real_r, window)\n",
        "    fake_vol = rolling_std(fake_r, window)\n",
        "    vol_loss = tf.reduce_mean(tf.abs(real_vol - fake_vol))\n",
        "\n",
        "    real_sq, fake_sq = tf.square(real_r), tf.square(fake_r)\n",
        "    real_acf = batched_acf(real_sq, 10)\n",
        "    fake_acf = batched_acf(fake_sq, 10)\n",
        "    cluster_loss = tf.reduce_mean(tf.square(real_acf - fake_acf))\n",
        "    return vol_loss + cluster_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Petc-du_-u-c"
      },
      "outputs": [],
      "source": [
        "def descale_time_varying(x_scaled, mu, sigma):\n",
        "    # x_scaled , mu , sigma  are all (B,T,F)\n",
        "    return x_scaled * sigma + mu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzoQeO92uSqY"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_conditional_generator_step(\n",
        "    X_mb: tf.Tensor,          # (B,T,F)  scaled real log‑returns\n",
        "    T_mb: tf.Tensor,          # (B,)     true lengths\n",
        "    Z_mb: tf.Tensor,          # (B,T,z)  latent noise\n",
        "    crisis_mb: tf.Tensor,     # (B,1)    crisis intensity targets\n",
        "    mu_mb,                    # (B,T,F) rolling means\n",
        "    sig_mb,                   # (B,T,F) rolling stds\n",
        "    *,\n",
        "    step: tf.Tensor,          # global step (int32/int64)\n",
        "    roll_window: int = 25,\n",
        "):\n",
        "    # ————————————————— weights (tune as you like) —————————————————\n",
        "    w_S, w_V      = 10.0, 10.0\n",
        "    w_mu, w_tail  = 15.0, 15.0\n",
        "    w_sigma, w_vol= 10.0, 20.0\n",
        "    w_acf, w_het  = 15.0, 20.0\n",
        "    w_C           = 1.0          # NEW: crisis intensity matching weight\n",
        "    w_crisis      = 0.5          # NEW: crisis-specific losses weight\n",
        "\n",
        "    mask = tf.expand_dims(tf.sequence_mask(T_mb, tf.shape(X_mb)[1], dtype=X_mb.dtype), -1)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # ---- forward ----\n",
        "        H_real = model.embedder_model(X_mb, training=False)\n",
        "        E_hat  = model.generator_model(Z_mb, crisis_mb, training=True)  # NOW CONDITIONAL\n",
        "        H_hat  = model.supervisor_model(E_hat, training=True)\n",
        "        H_sup  = model.supervisor_model(H_real, training=False)\n",
        "\n",
        "        # DUAL-HEAD discriminator outputs\n",
        "        Y_fake, crisis_pred_fake = model.discriminator_model(H_hat, training=False)\n",
        "\n",
        "        r_fake_scaled = model.recovery_model(H_hat, training=False) * mask\n",
        "\n",
        "        # --------- inverse-scale with time-varying stats ------------\n",
        "        r_real = descale_time_varying(X_mb*mask,  mu_mb,  sig_mb)\n",
        "        r_fake = descale_time_varying(r_fake_scaled, mu_mb, sig_mb)\n",
        "\n",
        "        # ===== ORIGINAL TimeGAN losses =====\n",
        "        G_loss_U = -tf.reduce_mean(Y_fake)\n",
        "        G_loss_S = tf.sqrt(tf.reduce_mean(tf.keras.losses.mse(H_real[:,1:,:], H_sup[:,:-1,:])))\n",
        "        G_loss_V = (tf.reduce_mean(tf.abs(tf.math.reduce_std(H_hat,0) - tf.math.reduce_std(H_real,0))) +\n",
        "                     tf.reduce_mean(tf.abs(tf.reduce_mean(H_hat,0) - tf.reduce_mean(H_real,0))))\n",
        "\n",
        "        # ===== ORIGINAL financial losses =====\n",
        "        # mean‑matching, VaR/ES, global σ\n",
        "        mu_r = tf.reduce_mean(r_real, axis=[0,1]); mu_f = tf.reduce_mean(r_fake, axis=[0,1])\n",
        "        L_mu = tf.reduce_mean(tf.square(mu_r - mu_f))\n",
        "        Tail_main = tail_loss_two_sided(r_real, r_fake, q=0.05, w_left=1.5, w_right=1.0)\n",
        "        sigma_r = tf.math.reduce_std(r_real); sigma_f = tf.math.reduce_std(r_fake)\n",
        "        margin = 1.1\n",
        "        L_sigma = tf.where(sigma_f > sigma_r*margin,\n",
        "                           (sigma_f-sigma_r)/sigma_r,\n",
        "                           (sigma_r*margin-sigma_f)/(sigma_r*0.5))\n",
        "        L_sigma = tf.reduce_mean(L_sigma)\n",
        "\n",
        "        # rolling σ + ACF\n",
        "        L_vol = tf.reduce_mean(tf.abs(rolling_std(r_real,roll_window,mask) -\n",
        "                                       rolling_std(r_fake,roll_window,mask)))\n",
        "\n",
        "        acf_real = batched_acf(r_real, 20); acf_fake = batched_acf(r_fake, 20)\n",
        "        acf_weights = tf.concat([[2.], tf.ones([19])],0)\n",
        "        L_acf = tf.reduce_mean(tf.abs(acf_real - acf_fake) * acf_weights[:,None,None])\n",
        "\n",
        "        # heteroskedasticity term\n",
        "        L_het = heteroskedasticity_loss(r_real, r_fake, window=20)\n",
        "\n",
        "        # ===== NEW C-G-GAN losses =====\n",
        "        # Crisis intensity matching loss\n",
        "        G_loss_C = tf.reduce_mean(tf.square(crisis_pred_fake - crisis_mb))\n",
        "\n",
        "        # Crisis-specific losses (enhanced tail risk, volatility clustering, correlations)\n",
        "        crisis_specific_loss = compute_crisis_specific_loss(r_real, r_fake, crisis_mb)\n",
        "\n",
        "        # scale-free diagnostics\n",
        "        dmu_over_sig = tf.reduce_mean(tf.abs(mu_r - mu_f) /\n",
        "                                      (sigma_r + 1e-8))\n",
        "        sig_ratio    = tf.reduce_mean(sigma_f / (sigma_r + 1e-8))\n",
        "\n",
        "        # ===== TOTAL LOSS (with C-G-GAN components) =====\n",
        "        G_total = (G_loss_U + w_S*G_loss_S + w_V*G_loss_V + w_mu*L_mu + w_tail*Tail_main +\n",
        "                   w_sigma*L_sigma + w_vol*L_vol + w_acf*L_acf + w_het*L_het +\n",
        "                   w_C*G_loss_C + w_crisis*crisis_specific_loss)  # NEW TERMS\n",
        "\n",
        "    vars_G = model.generator_model.trainable_variables + model.supervisor_model.trainable_variables\n",
        "    grads  = tape.gradient(G_total, vars_G)\n",
        "\n",
        "    model.optimizer_G.apply_gradients(zip(grads, vars_G))\n",
        "\n",
        "    return (G_total,\n",
        "            G_loss_U,  G_loss_S,  G_loss_V,\n",
        "            L_mu, Tail_main, L_sigma,\n",
        "            L_vol, L_acf, L_het,\n",
        "            G_loss_C, crisis_specific_loss,  # NEW RETURNS\n",
        "            dmu_over_sig, sig_ratio\n",
        "    )\n",
        "\n",
        "# ===== HELPER FUNCTION: Crisis-Specific Loss =====\n",
        "@tf.function\n",
        "def compute_crisis_specific_loss(r_real, r_fake, crisis_mb):\n",
        "    \"\"\"\n",
        "    Compute crisis-specific losses that focus on extreme events\n",
        "\n",
        "    Args:\n",
        "        r_real: (B,T,F) real returns\n",
        "        r_fake: (B,T,F) synthetic returns\n",
        "        crisis_mb: (B,1) crisis intensity levels\n",
        "    \"\"\"\n",
        "    # Weight samples by crisis intensity (higher weight for crisis periods)\n",
        "    crisis_weights = tf.expand_dims(crisis_mb, -1)  # (B,1,1)\n",
        "\n",
        "    # Enhanced tail risk matching (weighted by crisis intensity)\n",
        "    var_01 = tfp.stats.percentile(r_real, 1.0, axis=[0,1])   # 1% VaR\n",
        "    var_99 = tfp.stats.percentile(r_real, 99.0, axis=[0,1])  # 99% VaR\n",
        "\n",
        "    # Expected shortfall for extreme tails\n",
        "    es_real_01 = tf.reduce_mean(tf.boolean_mask(r_real, r_real <= var_01))\n",
        "    es_fake_01 = tf.reduce_mean(tf.boolean_mask(r_fake, r_fake <= var_01))\n",
        "    es_real_99 = tf.reduce_mean(tf.boolean_mask(r_real, r_real >= var_99))\n",
        "    es_fake_99 = tf.reduce_mean(tf.boolean_mask(r_fake, r_fake >= var_99))\n",
        "\n",
        "    tail_loss = (tf.square(es_real_01 - es_fake_01) +\n",
        "                 tf.square(es_real_99 - es_fake_99))\n",
        "\n",
        "    # Volatility clustering (crisis periods should have higher persistence)\n",
        "    vol_real = tf.abs(r_real)\n",
        "    vol_fake = tf.abs(r_fake)\n",
        "\n",
        "    # Autocorrelation of absolute returns (volatility clustering indicator)\n",
        "    vol_acf_real = batched_acf(vol_real, 10)\n",
        "    vol_acf_fake = batched_acf(vol_fake, 10)\n",
        "    vol_clustering_loss = tf.reduce_mean(tf.square(vol_acf_real - vol_acf_fake))\n",
        "\n",
        "    # Cross-asset correlation during crisis (should increase)\n",
        "    if tf.shape(r_real)[-1] > 1:  # Multi-asset case\n",
        "        # Compute rolling correlations\n",
        "        corr_real = compute_rolling_correlation(r_real, window=20)\n",
        "        corr_fake = compute_rolling_correlation(r_fake, window=20)\n",
        "        correlation_loss = tf.reduce_mean(tf.square(corr_real - corr_fake))\n",
        "    else:\n",
        "        correlation_loss = 0.0\n",
        "\n",
        "    # Maximum drawdown matching (important for crisis modeling)\n",
        "    dd_real = compute_max_drawdown(r_real)\n",
        "    dd_fake = compute_max_drawdown(r_fake)\n",
        "    drawdown_loss = tf.square(dd_real - dd_fake)\n",
        "\n",
        "    # Weight all losses by crisis intensity\n",
        "    total_crisis_loss = crisis_weights * (\n",
        "        2.0 * tail_loss +           # Heavy weight on tail events\n",
        "        1.5 * vol_clustering_loss + # Volatility clustering\n",
        "        1.0 * correlation_loss +    # Cross-asset correlations\n",
        "        1.0 * drawdown_loss         # Maximum drawdowns\n",
        "    )\n",
        "\n",
        "    return tf.reduce_mean(total_crisis_loss)\n",
        "\n",
        "@tf.function\n",
        "def compute_rolling_correlation(returns, window=20):\n",
        "    \"\"\"Compute rolling correlation between assets\"\"\"\n",
        "    if tf.shape(returns)[-1] < 2:\n",
        "        return tf.constant(0.0)\n",
        "\n",
        "    # Simple correlation between first two assets\n",
        "    r1 = returns[:, :, 0]  # (B,T)\n",
        "    r2 = returns[:, :, 1]  # (B,T)\n",
        "\n",
        "    # Rolling correlation using conv1d\n",
        "    r1_mean = tf.nn.avg_pool1d(tf.expand_dims(r1, -1), window, 1, \"SAME\")[:,:,0]\n",
        "    r2_mean = tf.nn.avg_pool1d(tf.expand_dims(r2, -1), window, 1, \"SAME\")[:,:,0]\n",
        "\n",
        "    r1_centered = r1 - r1_mean\n",
        "    r2_centered = r2 - r2_mean\n",
        "\n",
        "    cov = tf.nn.avg_pool1d(tf.expand_dims(r1_centered * r2_centered, -1), window, 1, \"SAME\")[:,:,0]\n",
        "    var1 = tf.nn.avg_pool1d(tf.expand_dims(r1_centered**2, -1), window, 1, \"SAME\")[:,:,0]\n",
        "    var2 = tf.nn.avg_pool1d(tf.expand_dims(r2_centered**2, -1), window, 1, \"SAME\")[:,:,0]\n",
        "\n",
        "    corr = cov / (tf.sqrt(var1 * var2) + 1e-8)\n",
        "    return tf.reduce_mean(corr)\n",
        "\n",
        "@tf.function\n",
        "def compute_max_drawdown(returns):\n",
        "    \"\"\"Compute maximum drawdown from returns\"\"\"\n",
        "    # Convert returns to cumulative wealth\n",
        "    cum_returns = tf.cumsum(returns, axis=1)\n",
        "\n",
        "    # Running maximum\n",
        "    running_max = tf.scan(\n",
        "        fn=lambda acc, x: tf.maximum(acc, x),\n",
        "        elems=tf.transpose(cum_returns, [1, 0, 2]),\n",
        "        initializer=cum_returns[:, 0, :]\n",
        "    )\n",
        "    running_max = tf.transpose(running_max, [1, 0, 2])\n",
        "\n",
        "    # Drawdown\n",
        "    drawdown = cum_returns - running_max\n",
        "    max_drawdown = tf.reduce_min(drawdown, axis=1)  # Most negative value\n",
        "\n",
        "    return tf.reduce_mean(max_drawdown)\n",
        "\n",
        "# ===== HELPER FUNCTION: Time-varying descaling =====\n",
        "@tf.function\n",
        "def descale_time_varying(x_scaled, mu_mb, sig_mb):\n",
        "    \"\"\"Descale using time-varying rolling statistics\"\"\"\n",
        "    return x_scaled * sig_mb + mu_mb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYiMeEEpIy8k"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_conditional_discriminator_step(X_mb, Z_mb, crisis_mb):\n",
        "    \"\"\"\n",
        "    One conditional critic update under the WGAN-SN setting with crisis regression.\n",
        "    The discriminator learns both adversarial classification and crisis intensity prediction.\n",
        "\n",
        "    Args:\n",
        "        X_mb: (B,T,F) real data batch\n",
        "        Z_mb: (B,T,z) noise batch\n",
        "        crisis_mb: (B,1) crisis intensity targets\n",
        "    \"\"\"\n",
        "\n",
        "    with tf.GradientTape() as disc_tape:\n",
        "        # ---------- real latent path ---------------------------------\n",
        "        H_real = tf.stop_gradient(model.embedder_model(X_mb))              # embedder is *frozen*\n",
        "        Y_real, crisis_pred_real = model.discriminator_model(H_real)       # DUAL outputs\n",
        "\n",
        "        # ---------- fake latent path  (GRADIENTS STOPPED here) -------\n",
        "        E_hat  = tf.stop_gradient(model.generator_model(Z_mb, crisis_mb))  # NOW CONDITIONAL\n",
        "        H_fake = tf.stop_gradient(model.supervisor_model(E_hat))\n",
        "        Y_fake, crisis_pred_fake = model.discriminator_model(H_fake)       # DUAL outputs\n",
        "\n",
        "        # ---------- Wasserstein adversarial loss --------------------\n",
        "        D_loss_adv = tf.reduce_mean(Y_fake) - tf.reduce_mean(Y_real)\n",
        "\n",
        "        # ---------- Crisis regression loss ---------------------------\n",
        "        # Real data: predict crisis intensity from real latent representations\n",
        "        D_loss_crisis_real = tf.reduce_mean(tf.square(crisis_pred_real - crisis_mb))\n",
        "\n",
        "        # Fake data: predict crisis intensity from synthetic latent representations\n",
        "        D_loss_crisis_fake = tf.reduce_mean(tf.square(crisis_pred_fake - crisis_mb))\n",
        "\n",
        "        # Combined crisis regression loss\n",
        "        D_loss_crisis = D_loss_crisis_real + D_loss_crisis_fake\n",
        "\n",
        "        # ---------- Total discriminator loss -------------------------\n",
        "        lambda_crisis = 1.0  # Weight for crisis regression loss\n",
        "        D_loss_total = D_loss_adv + lambda_crisis * D_loss_crisis\n",
        "\n",
        "    # Compute gradients for discriminator only\n",
        "    grads = disc_tape.gradient(\n",
        "        D_loss_total, model.discriminator_model.trainable_variables\n",
        "    )\n",
        "    grads, _ = tf.clip_by_global_norm(grads, 1.5)     # ‖g‖₂ ≤ 1.5\n",
        "\n",
        "    # Apply gradients to discriminator\n",
        "    model.optimizer_D.apply_gradients(\n",
        "        zip(grads, model.discriminator_model.trainable_variables)\n",
        "    )\n",
        "\n",
        "    return D_loss_total, D_loss_adv, D_loss_crisis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_cQOfz57OXw"
      },
      "outputs": [],
      "source": [
        "train_time, _ = extract_time(train_data)\n",
        "test_time , _ = extract_time(test_data)\n",
        "_, max_seq_len = extract_time(train_data)        # or use a constant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFTb3RlhZWe_"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def critic_grad_norm(x_real):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(x_real)  # Watch the input tensor\n",
        "        h = model.embedder_model(x_real)  # latent representation\n",
        "        y = model.discriminator_model(h)\n",
        "    grads = tape.gradient(y, h)  # Gradient of critic output wrt latent h\n",
        "    # Compute L2 norm over batch and time dimensions: shape (batch, time, features)\n",
        "    grad_norms = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2]) + 1e-8)\n",
        "    # Return average grad norm over batch\n",
        "    return tf.reduce_mean(grad_norms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amqSDfzUcLKG"
      },
      "outputs": [],
      "source": [
        "def generate_synthetic_samples(model,\n",
        "                               n_samples: int,\n",
        "                               ref_real: np.ndarray,\n",
        "                               max_len: int):\n",
        "    \"\"\"\n",
        "    Create `n_samples` synthetic sequences.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model      : your TimeGAN instance\n",
        "    n_samples  : how many sequences you want back\n",
        "    ref_real   : a *real* dataset of shape (N, L, F) –\n",
        "                 only used for their individual lengths\n",
        "    max_len    : integer, padding length you used in `random_generator`\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray of shape (n_samples, L_i, F) – the same\n",
        "    per-sequence lengths as in `ref_real[:n_samples]`\n",
        "    \"\"\"\n",
        "    # 1) grab the first n length values from whatever set you pass in\n",
        "    seq_lens = [len(seq) for seq in ref_real[:n_samples]]\n",
        "\n",
        "    synthetic_chunks = []\n",
        "    for i in range(0, n_samples, model.batch_size):\n",
        "        batch_lens = seq_lens[i : i + model.batch_size]\n",
        "\n",
        "        # build noise that matches those lengths\n",
        "        Z_mb = random_generator(len(batch_lens),\n",
        "                                model.z_dim,\n",
        "                                batch_lens,\n",
        "                                max_len)\n",
        "\n",
        "        # forward through G → S → R\n",
        "        E_hat = model.generator_model(Z_mb, training=False)\n",
        "        H_hat = model.supervisor_model(E_hat, training=False)\n",
        "        X_hat = model.recovery_model(H_hat, training=False)\n",
        "\n",
        "        synthetic_chunks.append(X_hat.numpy())\n",
        "\n",
        "    return np.concatenate(synthetic_chunks, axis=0)[:n_samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9U66jsYlMxa"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def critic_gap_no_grad(x_real, z_noise):\n",
        "    \"\"\" E[D_real] − E[D_fake]  (no gradients). \"\"\"\n",
        "    h_r = tf.stop_gradient(model.embedder_model(x_real))\n",
        "    h_f = tf.stop_gradient(model.supervisor_model(\n",
        "                               model.generator_model(z_noise)))\n",
        "    d_r = model.discriminator_model(h_r, training=False)\n",
        "    d_f = model.discriminator_model(h_f, training=False)\n",
        "    return tf.reduce_mean(d_r) - tf.reduce_mean(d_f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "init_sig = tf.cast(tf.math.reduce_std(train_data, axis=[0, 1]), tf.float32)\n",
        "vs_layer = VolatilityScaler(z_dim, init_sig=init_sig)    # ← pass here"
      ],
      "metadata": {
        "id": "gwC93lMOePcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlTGjBNC10Ft"
      },
      "outputs": [],
      "source": [
        "def train_conditional_timegan(model, ori_data, ori_time, rolling_stats, crisis_intensities):\n",
        "    \"\"\"\n",
        "    Complete training pipeline for Conditional TimeGAN with crisis modeling\n",
        "    (without discriminator and generator warm-up phases)\n",
        "\n",
        "    Args:\n",
        "        model: ConditionalTimeGAN instance\n",
        "        ori_data: preprocessed sequence data\n",
        "        ori_time: sequence lengths\n",
        "        rolling_stats: rolling standardization statistics\n",
        "        crisis_intensities: estimated crisis parameters for each sequence\n",
        "    \"\"\"\n",
        "    print(\"Starting Conditional TimeGAN training...\")\n",
        "\n",
        "    # Phase 1: Embedder-Recovery Pre-training (2000 iterations)\n",
        "    print(\"Phase 1: Training Embedder-Recovery...\")\n",
        "    for itt in range(20000):\n",
        "        X_mb, T_mb, mu_mb, sigma_mb, crisis_mb = batch_generator(\n",
        "            ori_data, ori_time,\n",
        "            rolling_stats['means'], rolling_stats['stds'],\n",
        "            crisis_intensities, model.batch_size\n",
        "        )\n",
        "        X_mb = tf.constant(X_mb, dtype=tf.float32)\n",
        "\n",
        "        E_loss = train_embedder(model, X_mb, T_mb)\n",
        "\n",
        "        if itt % 500 == 0:\n",
        "            print(f\"Iteration {itt}, Embedder Loss: {E_loss:.4f}\")\n",
        "\n",
        "    # Phase 2: Supervisor Pre-training (2000 iterations)\n",
        "    print(\"Phase 2: Training Supervisor...\")\n",
        "    for itt in range(20000):\n",
        "        X_mb, T_mb, mu_mb, sigma_mb, crisis_mb = batch_generator(\n",
        "            ori_data, ori_time,\n",
        "            rolling_stats['means'], rolling_stats['stds'],\n",
        "            crisis_intensities, model.batch_size\n",
        "        )\n",
        "        X_mb = tf.constant(X_mb, dtype=tf.float32)\n",
        "\n",
        "        S_loss = train_supervised_step(model, X_mb, T_mb)\n",
        "\n",
        "        if itt % 500 == 0:\n",
        "            print(f\"Iteration {itt}, Supervisor Loss: {S_loss:.4f}\")\n",
        "\n",
        "    # Phase 3: Joint Adversarial Training (main training loop)\n",
        "    print(\"Phase 3: Joint conditional adversarial training...\")\n",
        "    for itt in range(model.iterations):\n",
        "        # Train discriminator multiple times (2x per generator update)\n",
        "        d_losses = []\n",
        "        for _ in range(2):\n",
        "            X_mb, T_mb, mu_mb, sigma_mb, crisis_mb = batch_generator(\n",
        "                ori_data, ori_time,\n",
        "                rolling_stats['means'], rolling_stats['stds'],\n",
        "                crisis_intensities, model.batch_size\n",
        "            )\n",
        "            Z_mb = random_generator(model.batch_size, model.z_dim, T_mb, max(T_mb))\n",
        "\n",
        "            X_mb = tf.constant(X_mb, dtype=tf.float32)\n",
        "            Z_mb = tf.constant(Z_mb, dtype=tf.float32)\n",
        "            crisis_mb = tf.constant(crisis_mb, dtype=tf.float32)\n",
        "\n",
        "            D_loss, D_loss_adv, D_loss_crisis = train_conditional_discriminator_step(\n",
        "                X_mb, Z_mb, crisis_mb\n",
        "            )\n",
        "            d_losses.append(D_loss)\n",
        "\n",
        "        # Train generator once\n",
        "        X_mb, T_mb, mu_mb, sigma_mb, crisis_mb = batch_generator(\n",
        "            ori_data, ori_time,\n",
        "            rolling_stats['means'], rolling_stats['stds'],\n",
        "            crisis_intensities, model.batch_size\n",
        "        )\n",
        "        Z_mb = random_generator(model.batch_size, model.z_dim, T_mb, max(T_mb))\n",
        "\n",
        "        X_mb = tf.constant(X_mb, dtype=tf.float32)\n",
        "        Z_mb = tf.constant(Z_mb, dtype=tf.float32)\n",
        "        crisis_mb = tf.constant(crisis_mb, dtype=tf.float32)\n",
        "\n",
        "        G_losses = train_conditional_generator_step(\n",
        "            X_mb, Z_mb, T_mb, crisis_mb, mu_mb, sigma_mb, step=itt\n",
        "        )\n",
        "\n",
        "        # Unpack generator losses\n",
        "        (G_total, G_loss_U, G_loss_S, G_loss_V, L_mu, Tail_main, L_sigma,\n",
        "         L_vol, L_acf, L_het, G_loss_C, crisis_specific_loss,\n",
        "         dmu_over_sig, sig_ratio) = G_losses\n",
        "\n",
        "        # Periodic logging\n",
        "        if itt % 1000 == 0:\n",
        "            avg_d_loss = tf.reduce_mean(d_losses)\n",
        "            print(f\"\\nIteration {itt}\")\n",
        "            print(f\"  Discriminator - Total: {avg_d_loss:.4f}, Adv: {D_loss_adv:.4f}, Crisis: {D_loss_crisis:.4f}\")\n",
        "            print(f\"  Generator - Total: {G_total:.4f}\")\n",
        "            print(f\"    Adversarial: {G_loss_U:.4f}, Supervised: {G_loss_S:.4f}\")\n",
        "            print(f\"    Crisis Matching: {G_loss_C:.4f}, Crisis Specific: {crisis_specific_loss:.4f}\")\n",
        "            print(f\"    Financial - Mu: {L_mu:.4f}, Tail: {Tail_main:.4f}, Vol: {L_vol:.4f}\")\n",
        "            print(f\"    Diagnostics - μ/σ: {dmu_over_sig:.4f}, σ_ratio: {sig_ratio:.4f}\")\n",
        "\n",
        "        # Optional: Model checkpointing\n",
        "        if itt % 5000 == 0 and itt > 0:\n",
        "            print(f\"Saving checkpoint at iteration {itt}\")\n",
        "            # Save model weights here if needed\n",
        "\n",
        "    print(\"Conditional training completed!\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Conditional TimeGAN\n",
        "class ConditionalTimeGAN(TimeGAN):\n",
        "    def __init__(self, rolling_stats, crisis_intensities, **kwargs):\n",
        "        super().__init__(rolling_stats, **kwargs)\n",
        "        self.crisis_intensities = crisis_intensities\n",
        "\n",
        "        # Replace with conditional versions\n",
        "        self.generator_model = ConditionalGenerator(\n",
        "            hidden_dim=self.hidden_dim,\n",
        "            output_dim=self.hidden_dim,\n",
        "            num_layers=self.num_layers\n",
        "        )\n",
        "        self.discriminator_model = ConditionalDiscriminator(\n",
        "            hidden_dim=self.hidden_dim,\n",
        "            num_layers=self.num_layers\n",
        "        )\n",
        "\n",
        "        # Re-initialize weights\n",
        "        self._initialize_conditional_weights()\n",
        "\n",
        "    def _initialize_conditional_weights(self):\n",
        "        print(\"Initializing conditional model weights...\")\n",
        "        dummy_Z = tf.random.normal([1, 60, self.z_dim])\n",
        "        dummy_H = tf.random.normal([1, 60, self.hidden_dim])\n",
        "        dummy_c = tf.random.uniform([1, 1], 0.0, 1.0)\n",
        "\n",
        "        _ = self.generator_model(dummy_Z, dummy_c)\n",
        "        _ = self.discriminator_model(dummy_H)\n",
        "        print(\"Conditional model weights initialized.\")\n",
        "\n",
        "# Define conditional model\n",
        "model = ConditionalTimeGAN(rolling_stats, crisis_intensities)"
      ],
      "metadata": {
        "id": "QXbMuS3trg5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Estimate crisis intensities for your data\n",
        "    print(\"Estimating crisis intensities...\")\n",
        "    crisis_intensities, sigma_mins, sigma_maxs = preprocess_with_crisis_intensity(ori_data)\n",
        "    print(f\"Crisis intensities range: {crisis_intensities.min():.3f} to {crisis_intensities.max():.3f}\")\n",
        "\n",
        "    # Initialize conditional model\n",
        "    conditional_model = ConditionalTimeGAN(rolling_stats, crisis_intensities)\n",
        "\n",
        "    # Train the model\n",
        "    trained_conditional_model = train_conditional_timegan(\n",
        "        conditional_model, ori_data, ori_time, rolling_stats, crisis_intensities\n",
        "    )\n",
        "\n",
        "    print(\"Training completed successfully!\")"
      ],
      "metadata": {
        "id": "08uX6HOskACt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimate crisis intensities for your data\n",
        "print(\"Estimating crisis intensities...\")\n",
        "crisis_intensities, sigma_mins, sigma_maxs = preprocess_with_crisis_intensity(ori_data)\n",
        "print(f\"Crisis intensities range: {crisis_intensities.min():.3f} to {crisis_intensities.max():.3f}\")\n",
        "\n",
        "# Initialize and train conditional model\n",
        "conditional_model = ConditionalTimeGAN(rolling_stats, crisis_intensities)\n",
        "trained_conditional_model = train_conditional_timegan(\n",
        "    conditional_model, ori_data, ori_time, rolling_stats, crisis_intensities\n",
        ")\n",
        "\n",
        "def generate_conditional_scenarios(model, crisis_intensity, num_samples=100, seq_length=252):\n",
        "    \"\"\"Generate synthetic scenarios with specified crisis intensity\"\"\"\n",
        "    # Generate noise\n",
        "    Z_mb = np.zeros((num_samples, seq_length, model.z_dim))\n",
        "    T_mb = [seq_length] * num_samples\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        Z_mb[i, :, :] = t_noise((seq_length, model.z_dim), df=5, scale=0.6)\n",
        "\n",
        "    # Create crisis intensity tensor\n",
        "    crisis_tensor = np.full((num_samples, 1), crisis_intensity, dtype=np.float32)\n",
        "\n",
        "    Z_mb = tf.constant(Z_mb, dtype=tf.float32)\n",
        "    crisis_tensor = tf.constant(crisis_tensor, dtype=tf.float32)\n",
        "\n",
        "    # Generate synthetic latent representations\n",
        "    H_fake = model.generator_model(Z_mb, crisis_tensor)\n",
        "    H_hat_supervise = model.supervisor_model(H_fake)\n",
        "\n",
        "    # Convert back to return space\n",
        "    X_fake = model.recovery_model(H_hat_supervise)\n",
        "\n",
        "    return X_fake.numpy()\n",
        "\n",
        "# Example usage:\n",
        "print(\"Generating crisis scenarios...\")\n",
        "\n",
        "# Generate calm market scenarios (low crisis intensity)\n",
        "calm_scenarios = generate_conditional_scenarios(trained_conditional_model, 0.1, 50, seq_len)\n",
        "print(\"Calm scenarios shape:\", calm_scenarios.shape)\n",
        "\n",
        "# Generate crisis scenarios (high crisis intensity)\n",
        "crisis_scenarios = generate_conditional_scenarios(trained_conditional_model, 0.9, 50, seq_len)\n",
        "print(\"Crisis scenarios shape:\", crisis_scenarios.shape)\n",
        "\n",
        "# Compare statistics\n",
        "print(\"\\nCalm scenarios statistics:\")\n",
        "print(\"Mean:\", np.mean(calm_scenarios, axis=(0,1)))\n",
        "print(\"Std:\", np.std(calm_scenarios, axis=(0,1)))\n",
        "\n",
        "print(\"\\nCrisis scenarios statistics:\")\n",
        "print(\"Mean:\", np.mean(crisis_scenarios, axis=(0,1)))\n",
        "print(\"Std:\", np.std(crisis_scenarios, axis=(0,1)))\n"
      ],
      "metadata": {
        "id": "ICfURQtuhfmq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
